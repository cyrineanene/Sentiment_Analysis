# Use the official Spark image from the Docker Hub as the base image
FROM bitnami/spark:3.5.1

# Set the working directory in the Docker container
WORKDIR /app1

# Copy the Python script and other necessary files into the Docker container
COPY spark.py /app1/spark.py
COPY model_training.py /app1/model_training.py
COPY classifier.py /app1/classifier.py
COPY datasets/ /app1/datasets

# Install necessary Python psackages
RUN pip install pyspark==3.5.1 kafka-python

# Open necessary ports (Spark UI at 4040, master at 7077, worker at 8081)
# EXPOSE 4040 7077 8081

# Command to run on container start, here we are using the spark-submit to run your Python script
CMD ["spark-submit", "--master", "local[*]","--packages ","org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1", "/app1/spark.py"]

#after building the image, run this to run container: docker run -p 4040:4040 -p 7077:7077 -p 8081:8081 spark-sentiment-analysis 
#kafka, spark, model . bahdha build  => lezm ycommunikiw lezm network b les ports fi docum offici docker hub, spark master, spark worker, 
#lezm fi ekehr kol haaja nektb network w nafs esm baaed ekehr el page voir anas